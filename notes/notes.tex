\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, top=1in, bottom=1in, left=1in, right=1in]{geometry} 


\title{\textbf{Machine Learning Note}}
\author{Chauncey Liu}
\date{\today}

\begin{document}
 
\maketitle
 
\tableofcontents

\newpage
 
\section{Introduction}
\subsection{What is Machine Learning}
\subsection{Supervise Learning}
\subsection{Unsupervised Learning}

\section{Linear Regression with One Variable}
\subsection{Model and Cost Function}
\subsubsection{Model Representation}
\subsubsection{Cost Function}
\subsection{Parameter Learning}
\subsubsection{Gradient Descent}
\subsubsection{Gradient Descent Intuition}
\subsubsection{Gradient Descent For Linear Regression}


\section{Linear Regression with Multiple Variables}
\subsection{Multivariate Linear Regression}
\subsubsection{Multiple Features}
\subsubsection{Gradient Descent for Multiple Variables}
\subsubsection{Gradient in Practice I - Feature Scaling}
\subsubsection{Gradient in Practice II - Learning Rate}
\subsubsection{Features and Polynomial Regression}
\subsection{Computing Parameters Analytically}
\subsubsection{Normal Equation}
\subsubsection{Normal Equation Non-invertibility}

\section{Logistic Regression}
\subsection{Classification and Representation}
\subsubsection{Hypothesis Representation}
\subsubsection{Decision Boundary}
\subsection{Logistic Regression Model}
\subsubsection{Cost Function}
\subsubsection{Simplified Cost Function and Gradient Descent}
\subsubsection{Advanced Optimization}
\subsection{Multiclass Classification}
\subsubsection{Multiclass Classification: One-vs-all}

\newpage

\section{Regularization}
\subsection{Solving the Problem of Overfitting}
\subsubsection{The Problem of Overfitting}
\subsubsection{Cost Function}
\subsubsection{Regularized Linear Regression}
\subsubsection{Regularized Logistic Regression}

\section{Neural Networks: Representation}
\subsection{Motivation}
\subsubsection{Non-linear Hypotheses}
\subsubsection{Neurons and the Brain}
\subsection{Neural Networks}
\subsubsection{Model Representation I}
\subsubsection{Model Representation II}
\subsection{Applications}
\subsubsection{Examples and Intuitions I}
\subsubsection{Examples and Intuitions II}
\subsubsection{Multiclass Classification}

\section{Neural Networks: Learning}
\subsection{Cost Function and Backpropagation}
\subsubsection{Cost Function}
\subsubsection{Backpropagation Algorithm}
\subsubsection{Backpropagation Intution}
\subsection{Backpropagration in Practice}
\subsubsection{Implementation Note: Unrolling Parameters}
\subsubsection{Gradient Checking}
\subsubsection{Random Initialization}
\subsubsection{Putting It Together}
\subsection{Application of Neural Networks}
\subsubsection{Autonomous Driving}

\newpage

\section{Advice for Applying Machine Learning}
\subsection{Evaluating a Learning Algorithm}
\subsubsection{Deciding What to Try Next}
\subsubsection{Evaluating a Hypothesis}
\subsubsection{Model Selection and Train / Validation / Test Sets}
\subsection{Bias vs. Variance}
\subsubsection{Diagnosing Bias vs. Variance}
\subsubsection{Regularization and Bias / Variance}
\subsubsection{Learning Curves}
\subsubsection{Deciding What to do Next Revisited}


\section{Machine Learning System Design}
\subsection{Building a Spam CLassifier}
\subsubsection{Prioritizing What to Work On}
\subsubsection{Error Analysis}
\subsection{Handling Skewed Data}
\subsubsection{Error Metrics for Skewed Classes}
\subsubsection{Trading Off Precision and Recall}
\subsection{using Large Data Sets}
\subsubsection{Data For Machine Learning}

\end{document}